@article{ZHANG2019400,
title = {Stock Market Prediction Based on Generative Adversarial Network},
journal = {Procedia Computer Science},
volume = {147},
pages = {400-406},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.256},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302789},
author = {Kang Zhang and Guoqiang Zhong and Junyu Dong and Shengke Wang and Yong Wang},
keywords = {Deep Learning, Stock Prediction, Generative Adversarial Networks, Data Mining},
abstract = {Deep learning has recently achieved great success in many areas due to its strong capacity in data process. For instance, it has been widely used in financial areas such as stock market prediction, portfolio optimization, financial information processing and trade execution strategies. Stock market prediction is one of the most popular and valuable area in finance. In this paper, we propose a novel architecture of Generative Adversarial Network (GAN) with the Multi-Layer Perceptron (MLP) as the discriminator and the Long Short-Term Memory (LSTM) as the generator for forecasting the closing price of stocks. The generator is built by LSTM to mine the data distributions of stocks from given data in stock market and generate data in the same distributions, whereas the discriminator designed by MLP aims to discriminate the real stock data and generated data. We choose the daily data on S&P 500 Index and several stocks in a wide range of trading days and try to predict the daily closing price. Experimental results show that our novel GAN can get a promising performance in the closing price prediction on the real data compared with other models in machine learning and deep learning.}
}
@misc{BibEntry2021Nov,
	title = {{xn--vjqzdz38azjct59bq4kok2aj36qka{\ifmmode---\else\textemdash\fi}{\ifmmode---\else\textemdash\fi}xn--jhqu4a64dipjbd305as4sjpkt29cxcya5ew2eqi1h}},
	journal = {xn--ohq7e440fipo},
	year = {2021},
	month = {Nov},
	note = {[Online; accessed 25. Nov. 2021]},
	url = {https://zhuanlan.zhihu.com/p/136656546}
}
@article{https://doi.org/10.1111/j.2153-3490.1970.tb01933.x,
author = {SUNDQVIST, HILDING and VERONIS, GEORGE},
title = {A simple finite-difference grid with non-constant intervals},
journal = {Tellus},
volume = {22},
number = {1},
pages = {26-31},
doi = {https://doi.org/10.1111/j.2153-3490.1970.tb01933.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2153-3490.1970.tb01933.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2153-3490.1970.tb01933.x},
abstract = {ABSTRACT A finite difference network with non-uniform grid intervals, hi, such that hi = hi-1 + O(h2i-1) is presented. The use of the grid network for two specific choices of hi - hi-1 for a linear boundary layer problem shows that it has distinct advantages in computational efficiency and in accuracy of representation of the analytical solution.},
year = {1970}
}


@article{Alan_D_Sokal_2011,
   title={A Really Simple Elementary Proof of the Uniform Boundedness Theorem},
   volume={118},
   ISSN={0002-9890},
   url={http://dx.doi.org/10.4169/amer.math.monthly.118.05.450},
   DOI={10.4169/amer.math.monthly.118.05.450},
   number={5},
   journal={The American Mathematical Monthly},
   publisher={Informa UK Limited},
   author={Alan D. Sokal},
   year={2011},
   pages={450}
}

@book{Boucheron2013_conineq,
	author = {Boucheron Stéphane, Lugosi Gábor, Massart Pascal},
	title = {Concentration Inequalities: A Nonasymptotic Theory of Independence},
	date = {2013},
	OPTpagetotal = {496},
	OPTdoi = {10.1093/acprof:oso/9780199535255.001.0001},
}

@article{deep_learning_as_ker,
	author={Pedro Domingos},
	title={Every Model Learned by Gradient Descent Is Approximately a Kernel Machine},
	date={2020}
}

@article{li_chebyshev-type_2011,
	title = {Chebyshev-type methods and preconditioning techniques},
	volume = {218},
	issn = {00963003},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0096300311007053},
	doi = {10.1016/j.amc.2011.05.036},
	abstract = {Recently, a Newton’s iterative method is attracting more and more attention from various ﬁelds of science and engineering. This method is generally quadratically convergent. In this paper, some Chebyshev-type methods with the third order convergence are analyzed in detail and used to compute approximate inverse preconditioners for solving the linear system Ax = b. Theoretic analysis and numerical experiments show that Chebyshev’s method is more effective than Newton’s one in the case of constructing approximate inverse preconditioners.},
	pages = {260--270},
	number = {2},
	journaltitle = {Applied Mathematics and Computation},
	shortjournal = {Applied Mathematics and Computation},
	author = {Li, Hou-Biao and Huang, Ting-Zhu and Zhang, Yong and Liu, Xing-Ping and Gu, Tong-Xiang},
	urldate = {2021-03-16},
	date = {2011-09},
	langid = {english}
}

@article{distribution-of-local-minima-of-graph,
title={On the Distribution of the Number of Local Minima of a Random Function on a Graph},
author={Pierre Baldi, Yosef Rinott, Charles Stein},
journaltitle={ Advances in Neural Information Processing Systems 2},
shortjournal={NIPS1989}
}

@misc{loshchilov2019decoupled,
	title={Decoupled Weight Decay Regularization}, 
	author={Ilya Loshchilov and Frank Hutter},
	year={2019},
	eprint={1711.05101},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{goodfellow2014generative,
	title={Generative Adversarial Networks}, 
	author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
	year={2014},
	eprint={1406.2661},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{10.2307.3215821,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3215821},
 abstract = {The maximum drawdown at time T of a random process on [0, T] can be defined informally as the largest drop from a peak to a trough. In this paper, we investigate the behaviour of this statistic for a Brownian motion with drift. In particular, we give an infinite series representation of its distribution and consider its expected value. When the drift is zero, we give an analytic expression for the expected value, and for nonzero drift, we give an infinite series representation. For all cases, we compute the limiting (T → ∞) behaviour, which can be logarithmic (for positive drift), square root (for zero drift) or linear (for negative drift).},
 author = {Malik Magdon-Ismail and Amir F. Atiya and Amrit Pratap and Yaser S. Abu-Mostafa},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {147--161},
 publisher = {Applied Probability Trust},
 title = {On the Maximum Drawdown of a Brownian Motion},
 volume = {41},
 year = {2004}
}

